{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b078ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:            x86_64\n",
      "  CPU op-mode(s):        32-bit, 64-bit\n",
      "  Address sizes:         39 bits physical, 48 bits virtual\n",
      "  Byte Order:            Little Endian\n",
      "CPU(s):                  24\n",
      "  On-line CPU(s) list:   0-23\n",
      "Vendor ID:               GenuineIntel\n",
      "  Model name:            13th Gen Intel(R) Core(TM) i7-13700KF\n",
      "    CPU family:          6\n",
      "    Model:               183\n",
      "    Thread(s) per core:  2\n",
      "    Core(s) per socket:  12\n",
      "    Socket(s):           1\n",
      "    Stepping:            1\n",
      "    BogoMIPS:            6835.19\n",
      "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
      "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscal\n",
      "                         l nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopo\n",
      "                         logy tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3\n",
      "                          fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c\n",
      "                          rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs \n",
      "                         ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 e\n",
      "                         rms invpcid rdseed adx smap clflushopt clwb sha_ni xsav\n",
      "                         eopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rd\n",
      "                         pid fsrm md_clear flush_l1d arch_capabilities\n",
      "Virtualization features: \n",
      "  Hypervisor vendor:     Microsoft\n",
      "  Virtualization type:   full\n",
      "Caches (sum of all):     \n",
      "  L1d:                   576 KiB (12 instances)\n",
      "  L1i:                   384 KiB (12 instances)\n",
      "  L2:                    24 MiB (12 instances)\n",
      "  L3:                    30 MiB (1 instance)\n",
      "Vulnerabilities:         \n",
      "  Gather data sampling:  Not affected\n",
      "  Itlb multihit:         Not affected\n",
      "  L1tf:                  Not affected\n",
      "  Mds:                   Not affected\n",
      "  Meltdown:              Not affected\n",
      "  Mmio stale data:       Unknown: No mitigations\n",
      "  Retbleed:              Mitigation; Enhanced IBRS\n",
      "  Spec rstack overflow:  Not affected\n",
      "  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\n",
      "                          and seccomp\n",
      "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\n",
      "                          sanitization\n",
      "  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB fillin\n",
      "                         g, PBRSB-eIBRS SW sequence\n",
      "  Srbds:                 Not affected\n",
      "  Tsx async abort:       Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd09667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1070', major=6, minor=1, total_memory=8191MB, multi_processor_count=15, uuid=ce792cd3-464b-1176-c4ca-2ecf6eef4afa, L2_cache_size=2MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1be48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.3\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import os, transformers \n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# set cache directory out of $HOME to $WORK\n",
    "default_cache_dir = \".cache/\"\n",
    "os.environ[\"HF_HOME\"] = default_cache_dir\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237b127",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de71fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "}) \n",
      "\n",
      "Astronomia é uma ciência natural que estuda corpos celestes (como estrelas, planetas, cometas, nebulosas, aglomerados de estrelas, galáxias) e fenômenos que se originam fora da atmosfera da Terra (como a radiação cósmica de fundo em micro-ondas). Preocupada com a evolução, a física e a química de objetos celestes, bem como a formação e o desenvolvimento do universo.\n",
      "\n",
      "A astronomia é uma das mais antigas ciências. Culturas pré-históricas deixaram registrados vários artefatos astronômicos, como Stonehenge, os montes de Newgrange e os menires. As primeiras civilizações, como os babilônios, gregos, chineses, indianos, persas e maias realizaram observações metódicas do céu noturno. No entanto, a invenção do telescópio permitiu o desenvolvimento da astronomia moderna. Historicamente, a astronomia incluiu disciplinas tão diversas como astrometria, navegação astronômica, astronomia observacional e a elaboração de calendários. Durante o período medieval, seu estudo era obrigatório e estava inclu\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \n",
    "                         \"20231101.pt\", \n",
    "                         split=\"train[:1_000]\", \n",
    "                         num_proc=cpu_count(),\n",
    "                         cache_dir=default_cache_dir\n",
    "                         )\n",
    "\n",
    "wikipedia = wikipedia.remove_columns([col for col in wikipedia.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "print(wikipedia, '\\n')\n",
    "\n",
    "print(wikipedia[0][\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e45695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# raw_datasets = concatenate_datasets([wikipedia, brwac])\n",
    "raw_datasets = concatenate_datasets([wikipedia])\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c55fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82087ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'num_words', 'stopwords', 'average'],\n",
       "    num_rows: 74024\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analize_texts(row):\n",
    "\n",
    "    list_para = []\n",
    "    list_words = []\n",
    "    list_stopwords = []\n",
    "    list_average = []\n",
    "\n",
    "    for doc in row[\"text\"]:\n",
    "        for paragraph in doc.split('\\n'):\n",
    "\n",
    "            # strip whitespaces\n",
    "            paragraph = paragraph.strip()\n",
    "\n",
    "            # skip single or empty worded paragraphs\n",
    "            if (len(paragraph.split()) < 2):\n",
    "                continue\n",
    "\n",
    "            # count how many stopwords are in the paragraph\n",
    "            stopwords_cnt = 0    \n",
    "            for word in paragraph.split():\n",
    "                for stop in stopwords:\n",
    "                    if stop.casefold() == word.casefold():  # insensitive case\n",
    "                        stopwords_cnt += 1\n",
    "                        break # count once and speed up everything\n",
    "            \n",
    "            # count non whitespace characters\n",
    "            characters = 0\n",
    "            for word in paragraph.split():\n",
    "                characters += len(word)\n",
    "\n",
    "            list_para.append(paragraph)\n",
    "            list_words.append(len(paragraph.split()))\n",
    "            list_stopwords.append(stopwords_cnt)\n",
    "            list_average.append(characters/len(paragraph.split()))\n",
    "\n",
    "    return {\"paragraphs\" : list_para, \"num_words\" : list_words, \"stopwords\" : list_stopwords, \"average\" : list_average}\n",
    "\n",
    "preprocessed_datasets = raw_datasets.map(analize_texts,\n",
    "                                         batched = True,\n",
    "                                         remove_columns=[\"text\"],\n",
    "                                         num_proc = cpu_count(),\n",
    "                                        )\n",
    "\n",
    "preprocessed_datasets = preprocessed_datasets.rename_column(\"paragraphs\", \"text\")\n",
    "\n",
    "preprocessed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96641696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'num_words', 'stopwords', 'average'],\n",
       "    num_rows: 42945\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_datasets = preprocessed_datasets.filter(\n",
    "    lambda example: \n",
    "            example[\"num_words\"] >= 10 \n",
    "        and example[\"num_words\"] <= 512\n",
    "        and example[\"stopwords\"] >= 1\n",
    "        and example[\"average\"] >= 2 \n",
    "        and example[\"average\"] <= 15\n",
    "    )\n",
    "\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953f0031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_031_590\n"
     ]
    }
   ],
   "source": [
    "total_words_corpus = sum( filtered_datasets[\"num_words\"] )\n",
    "\n",
    "print(f\"{total_words_corpus:_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7769c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'num_words', 'stopwords', 'average'],\n",
       "        num_rows: 40797\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'num_words', 'stopwords', 'average'],\n",
       "        num_rows: 2148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = filtered_datasets.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4115a",
   "metadata": {},
   "source": [
    "## Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc540f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='answerdotai/ModernBERT-base', vocab_size=50280, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"|||IP_ADDRESS|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"|||EMAIL_ADDRESS|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"|||PHONE_NUMBER|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50280: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50281: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50282: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50283: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50284: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t50285: AddedToken(\"[unused0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"[unused1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"[unused2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"[unused3]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"[unused4]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"[unused5]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"[unused6]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"[unused7]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"[unused8]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"[unused9]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50295: AddedToken(\"[unused10]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50296: AddedToken(\"[unused11]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50297: AddedToken(\"[unused12]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50298: AddedToken(\"[unused13]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50299: AddedToken(\"[unused14]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50300: AddedToken(\"[unused15]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50301: AddedToken(\"[unused16]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50302: AddedToken(\"[unused17]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50303: AddedToken(\"[unused18]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50304: AddedToken(\"[unused19]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50305: AddedToken(\"[unused20]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50306: AddedToken(\"[unused21]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50307: AddedToken(\"[unused22]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50308: AddedToken(\"[unused23]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50309: AddedToken(\"[unused24]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50310: AddedToken(\"[unused25]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50311: AddedToken(\"[unused26]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50312: AddedToken(\"[unused27]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50313: AddedToken(\"[unused28]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50314: AddedToken(\"[unused29]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50315: AddedToken(\"[unused30]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50316: AddedToken(\"[unused31]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50317: AddedToken(\"[unused32]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50318: AddedToken(\"[unused33]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50319: AddedToken(\"[unused34]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50320: AddedToken(\"[unused35]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50321: AddedToken(\"[unused36]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50322: AddedToken(\"[unused37]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50323: AddedToken(\"[unused38]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50324: AddedToken(\"[unused39]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50325: AddedToken(\"[unused40]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50326: AddedToken(\"[unused41]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50327: AddedToken(\"[unused42]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50328: AddedToken(\"[unused43]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50329: AddedToken(\"[unused44]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50330: AddedToken(\"[unused45]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50331: AddedToken(\"[unused46]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50332: AddedToken(\"[unused47]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50333: AddedToken(\"[unused48]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50334: AddedToken(\"[unused49]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50335: AddedToken(\"[unused50]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50336: AddedToken(\"[unused51]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50337: AddedToken(\"[unused52]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50338: AddedToken(\"[unused53]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50339: AddedToken(\"[unused54]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50340: AddedToken(\"[unused55]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50341: AddedToken(\"[unused56]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50342: AddedToken(\"[unused57]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50343: AddedToken(\"[unused58]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50344: AddedToken(\"[unused59]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50345: AddedToken(\"[unused60]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50346: AddedToken(\"[unused61]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50347: AddedToken(\"[unused62]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50348: AddedToken(\"[unused63]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50349: AddedToken(\"[unused64]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50350: AddedToken(\"[unused65]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50351: AddedToken(\"[unused66]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50352: AddedToken(\"[unused67]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50353: AddedToken(\"[unused68]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50354: AddedToken(\"[unused69]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50355: AddedToken(\"[unused70]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50356: AddedToken(\"[unused71]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50357: AddedToken(\"[unused72]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50358: AddedToken(\"[unused73]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50359: AddedToken(\"[unused74]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50360: AddedToken(\"[unused75]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50361: AddedToken(\"[unused76]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50362: AddedToken(\"[unused77]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50363: AddedToken(\"[unused78]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50364: AddedToken(\"[unused79]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50365: AddedToken(\"[unused80]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50366: AddedToken(\"[unused81]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50367: AddedToken(\"[unused82]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "modern_tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "\n",
    "modern_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b346f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i: i + batch_size][\"text\"]\n",
    "\n",
    "tokenizer = modern_tokenizer.train_new_from_iterator(\n",
    "    text_iterator = batch_iterator(split_dataset[\"train\"]),\n",
    "    vocab_size = modern_tokenizer.vocab_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c24e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='answerdotai/ModernBERT-base', vocab_size=50280, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb6f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 205, 6370, 919, 413, 280, 11268, 343, 10329, 18, 641, 280, 10329, 477, 4441, 205, 4]\n",
      "['[CLS]', 'Ċ', 'At', 'ira', 'ram', 'Ġo', 'Ġpau', 'Ġno', 'Ġgato', ',', 'Ġmas', 'Ġo', 'Ġgato', 'ĠnÃ£o', 'Ġmorreu', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 7533, 46039, 18, 1608, 46039, 313, 442, 1191, 5307, 205, 4]\n",
      "['[CLS]', 'Ċ', 'NÃ£o', 'Ġsei', ',', 'ĠsÃ³', 'Ġsei', 'Ġque', 'Ġfoi', 'Ġassim', '...', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 58, 1234, 346, 280, 2135, 1471, 85, 18, 264, 19646, 303, 280, 2135, 20666, 271, 681, 351, 30533, 1167, 6542, 205, 4]\n",
      "['[CLS]', 'Ċ', 'T', 'esta', 'ndo', 'Ġo', 'Ġmodo', 'Ġcontinu', 'o', ',', 'Ġe', 'Ġtamb', 'em', 'Ġo', 'Ġmodo', 'Ġsubju', 'nt', 'ivo', 'Ġ(', 'Ġsoub', 'esse', 'Ġ)', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 80, 8577, 18, 20623, 18, 7202, 18, 12436, 8577, 18, 12436, 364, 1156, 18, 15026, 18, 3270, 205, 4]\n",
      "['[CLS]', 'Ċ', 'j', 'usto', ',', 'Ġjusta', ',', 'ĠjustiÃ§a', ',', 'Ġinj', 'usto', ',', 'Ġinj', 'us', 'tamente', ',', 'Ġjustamente', ',', 'Ġjunto', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 27769, 14858, 346, 542, 1853, 18, 3632, 313, 3723, 369, 542, 1853, 19855, 265, 542, 497, 6547, 305, 9434, 2316, 37, 227, 205, 4]\n",
      "['[CLS]', 'Ċ', 'ĠĠĠ', 'Ġtesta', 'ndo', 'Ġac', 'entos', ',', 'ĠserÃ¡', 'Ġque', 'Ġmanter', 'Ġos', 'Ġac', 'entos', 'Ġmelhora', 'Ġa', 'Ġac', 'ur', 'Ã¡cia', 'Ġdo', 'Ġmeu', 'Ġmodelo', '?', 'Ġ', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 27769, 6455, 32, 862, 994, 1371, 18, 46887, 18, 862, 994, 800, 18, 24619, 284, 18, 46887, 736, 18, 24619, 5172, 736, 205, 4]\n",
      "['[CLS]', 'Ċ', 'ĠĠĠ', 'Ġamigo', ':', 'Ġam', 'igu', 'inho', ',', 'Ġamiga', ',', 'Ġam', 'igu', 'inha', ',', 'Ġamig', 'Ã£o', ',', 'Ġamiga', 'Ã§o', ',', 'Ġamig', 'alha', 'Ã§o', 'Ċ', '[SEP]']\n",
      "\n",
      "[3, 205, 27769, 9434, 11224, 268, 24394, 357, 1408, 20, 2181, 20, 2181, 20, 2181, 21, 4053, 363, 265, 10316, 758, 10412, 205, 4]\n",
      "['[CLS]', 'Ċ', 'ĠĠĠ', 'Ġmeu', 'ĠendereÃ§o', 'Ġde', 'Ġip', 'ĠÃ©', 'Ġ10', '.', '10', '.', '10', '.', '10', '/', '24', 'Ġpara', 'Ġa', 'Ġminha', 'Ġsub', 'net', 'Ċ', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer(sample):\n",
    "    encoding = tokenizer.encode(sample)\n",
    "    print(encoding)\n",
    "    print(tokenizer.convert_ids_to_tokens(encoding))\n",
    "    print()\n",
    "\n",
    "test_tokenizer('''\n",
    "Atiraram o pau no gato, mas o gato não morreu\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Não sei, só sei que foi assim...\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Testando o modo continuo, e tambem o modo subjuntivo ( soubesse )\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "justo, justa, justiça, injusto, injustamente, justamente, junto\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    testando acentos, será que manter os acentos melhora a acurácia do meu modelo? \n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    amigo: amiguinho, amiga, amiguinha, amigão, amigaço, amigalhaço\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    meu endereço de ip é 10.10.10.10/24 para a minha subnet\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94bb76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (0, 1)),\n",
       " ('ĠpopulaÃ§Ã£o', (1, 11)),\n",
       " ('Ġtotal', (11, 17)),\n",
       " ('Ġda', (17, 20)),\n",
       " ('ĠAmÃ©rica', (20, 28)),\n",
       " ('Ġera', (28, 32)),\n",
       " ('Ġde', (32, 35)),\n",
       " ('Ġ', (35, 36)),\n",
       " ('Ġhabitantes', (36, 47)),\n",
       " ('Ġsegundo', (47, 55)),\n",
       " ('Ġestimativas', (55, 67)),\n",
       " ('Ġde', (67, 70)),\n",
       " ('Ġ2008', (70, 75)),\n",
       " ('.', (75, 76)),\n",
       " ('ĠA', (76, 78)),\n",
       " ('ĠpopulaÃ§Ã£o', (78, 88)),\n",
       " ('Ġda', (88, 91)),\n",
       " ('ĠAmÃ©rica', (91, 99)),\n",
       " ('Ġcompreende', (99, 110)),\n",
       " ('Ġdescendentes', (110, 123)),\n",
       " ('Ġde', (123, 126)),\n",
       " ('Ġgrandes', (126, 134)),\n",
       " ('Ġgrupos', (134, 141)),\n",
       " ('ĠÃ©tnicos', (141, 149)),\n",
       " (',', (149, 150)),\n",
       " ('Ġcomo', (150, 155)),\n",
       " ('Ġos', (155, 158)),\n",
       " ('ĠindÃŃgenas', (158, 168)),\n",
       " ('Ġ(', (168, 170)),\n",
       " ('inclusive', (170, 179)),\n",
       " ('ĠinuÃŃtes', (179, 187)),\n",
       " ('Ġe', (187, 189)),\n",
       " ('ĠaleÃºtas', (189, 197)),\n",
       " ('),', (197, 199)),\n",
       " ('Ġos', (199, 202)),\n",
       " ('Ġeuropeus', (202, 211)),\n",
       " ('Ġ(', (211, 213)),\n",
       " ('principalmente', (213, 227)),\n",
       " ('ĠespanhÃ³is', (227, 237)),\n",
       " (',', (237, 238)),\n",
       " ('Ġingleses', (238, 247)),\n",
       " (',', (247, 248)),\n",
       " ('Ġirlandeses', (248, 259)),\n",
       " (',', (259, 260)),\n",
       " ('Ġitalianos', (260, 270)),\n",
       " (',', (270, 271)),\n",
       " ('Ġportugueses', (271, 283)),\n",
       " (',', (283, 284)),\n",
       " ('Ġfranceses', (284, 294)),\n",
       " (',', (294, 295)),\n",
       " ('ĠalemÃ£es', (295, 303)),\n",
       " ('Ġe', (303, 305)),\n",
       " ('Ġneerlandeses', (305, 318)),\n",
       " ('),', (318, 320)),\n",
       " ('Ġnegros', (320, 327)),\n",
       " ('Ġafricanos', (327, 337)),\n",
       " (',', (337, 338)),\n",
       " ('ĠasiÃ¡ticos', (338, 348)),\n",
       " ('Ġ(', (348, 350)),\n",
       " ('como', (350, 354)),\n",
       " ('Ġos', (354, 357)),\n",
       " ('Ġamarelos', (357, 366)),\n",
       " ('Ġe', (366, 368)),\n",
       " ('Ġos', (368, 371)),\n",
       " ('ĠmÃ©dio', (371, 377)),\n",
       " ('-', (377, 378)),\n",
       " ('orientais', (378, 387)),\n",
       " ('),', (387, 389)),\n",
       " ('Ġbem', (389, 393)),\n",
       " ('Ġcomo', (393, 398)),\n",
       " ('ĠmestiÃ§os', (398, 407)),\n",
       " ('Ġe', (407, 409)),\n",
       " ('Ġmulatos', (409, 417)),\n",
       " ('.', (417, 418))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_and_pre_tokenize(text):\n",
    "    normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\n",
    "    processed = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized)\n",
    "    return processed\n",
    "\n",
    "normalize_and_pre_tokenize( split_dataset[\"train\"][0][\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203b6d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num_words', 'stopwords', 'average', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 40797\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['num_words', 'stopwords', 'average', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 2148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "\n",
    "tokenized_datasets = split_dataset.map(group_texts, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      # num_proc=cpu_count()\n",
    "                                      )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af40c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tokenized_datasets[\"train\"]\n",
    "evaluation_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf127fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertConfig {\n",
       "  \"architectures\": [\n",
       "    \"ModernBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 50281,\n",
       "  \"classifier_activation\": \"gelu\",\n",
       "  \"classifier_bias\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"classifier_pooling\": \"mean\",\n",
       "  \"cls_token_id\": 50281,\n",
       "  \"decoder_bias\": true,\n",
       "  \"deterministic_flash_attn\": false,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 50282,\n",
       "  \"global_attn_every_n_layers\": 3,\n",
       "  \"global_rope_theta\": 160000.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_activation\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_cutoff_factor\": 2.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1152,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"local_attention\": 128,\n",
       "  \"local_rope_theta\": 10000.0,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mlp_dropout\": 0.0,\n",
       "  \"model_type\": \"modernbert\",\n",
       "  \"norm_bias\": false,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"pad_token_id\": 50283,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"repad_logits_with_grad\": false,\n",
       "  \"sep_token_id\": 50282,\n",
       "  \"sparse_pred_ignore_index\": -100,\n",
       "  \"sparse_prediction\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.53.3\",\n",
       "  \"vocab_size\": 50368\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertConfig\n",
    "\n",
    "config = ModernBertConfig.from_pretrained(\"answerdotai/ModernBERT-base\", reference_compile=False)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408206bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertConfig {\n",
       "  \"architectures\": [\n",
       "    \"ModernBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 50281,\n",
       "  \"classifier_activation\": \"gelu\",\n",
       "  \"classifier_bias\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"classifier_pooling\": \"mean\",\n",
       "  \"cls_token_id\": 50281,\n",
       "  \"decoder_bias\": true,\n",
       "  \"deterministic_flash_attn\": false,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 50282,\n",
       "  \"global_attn_every_n_layers\": 3,\n",
       "  \"global_rope_theta\": 160000.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_activation\": \"gelu\",\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_cutoff_factor\": 2.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"local_attention\": 128,\n",
       "  \"local_rope_theta\": 10000.0,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mlp_dropout\": 0.0,\n",
       "  \"model_type\": \"modernbert\",\n",
       "  \"norm_bias\": false,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"pad_token_id\": 50283,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"repad_logits_with_grad\": false,\n",
       "  \"sep_token_id\": 50282,\n",
       "  \"sparse_pred_ignore_index\": -100,\n",
       "  \"sparse_prediction\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.53.3\",\n",
       "  \"vocab_size\": 50368\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size = 128\n",
    "config.intermediate_size = 256\n",
    "config.num_attention_heads = 4\n",
    "config.num_hidden_layers = 8\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a654ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  7826880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(50368, 128, padding_idx=50283)\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=50368, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ModernBertForMaskedLM\n",
    "\n",
    "model = ModernBertForMaskedLM(config=config)\n",
    "\n",
    "print(\"parameters: \", model.num_parameters())\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32799fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mask 30% of the tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm = True,\n",
    "    mlm_probability=0.3\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35983c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"ModBertBr/{10.0}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e35bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, get_wsd_schedule\n",
    "from torch.optim import AdamW\n",
    "\n",
    "total_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'training/{model_name}',\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # num_train_epochs=1,                     # number of training epochs\n",
    "    max_steps=total_steps,\n",
    "    # max_steps=100,\n",
    "\n",
    "    gradient_accumulation_steps = 1,\n",
    "    # eval_accumulation_steps = 1,\n",
    "\n",
    "    per_device_train_batch_size=4,          # batch size for training\n",
    "    # per_device_eval_batch_size=32,           # batch size for evaluation\n",
    "\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True, # output the initial loss\n",
    "    logging_steps=1_000,\n",
    "    # logging_dir=f\"training-logs/{model_name}\",\n",
    "    # report_to=[\"tensorboard\"],\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1_000,                      # Save checkpoints every 100 steps\n",
    "    save_total_limit=5,                  # Limit the total number of saved checkpoints\n",
    "\n",
    "    fp16=True,                            # Enable mixed precision for faster training\n",
    ")\n",
    "\n",
    "# Create default optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 8e-4,\n",
    "    weight_decay=1e-2,\n",
    "    betas = (0.9, 0.999),\n",
    ")\n",
    "\n",
    "scheduler = get_wsd_schedule(\n",
    "    optimizer=optimizer,                  # Your optimizer\n",
    "    num_warmup_steps=total_steps * 0.1,   # Number of warmup steps\n",
    "    num_stable_steps=total_steps * 0.8,   # Number of stable steps\n",
    "    num_decay_steps=total_steps * 0.1,   # Number of decay steps\n",
    "    warmup_type=\"linear\",   # Warmup type\n",
    "    decay_type=\"1-sqrt\",    # Decay type\n",
    "    num_cycles=0.5,         # Number of cosine cycles\n",
    "    min_lr_ratio=0.0,       # Minimum learning rate ratio\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # Model to train\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=training_dataset,     # Training dataset\n",
    "    # eval_dataset=evaluation_dataset,  # Evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv (:D)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
